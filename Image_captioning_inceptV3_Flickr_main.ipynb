{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Image_captioning_inceptV3_Flickr_main.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anas321/ImageCaptioning/blob/master/Image_captioning_inceptV3_Flickr_main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpjG_SnPoJSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03939216-31b6-48e5-ff73-260acf53de84"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhRtvCUavjKN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "2cb7af0d-fa17-448a-d729-adfe50594bab"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9JPungsup7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec68b0e-d9df-4c92-a478-b2502100ba4b"
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/ImageCaptioning/InceptV3/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ImageCaptioning/InceptV3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO246bz9iDcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1afeabc1-5637-43e3-fe20-c56c17e8c5b4"
      },
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfP2X7Ym8L3Q"
      },
      "source": [
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoNg2t3SIEGl"
      },
      "source": [
        "# uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JR6VIBSmPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b19897e-229a-435b-932b-21f1d66e7d5f"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mannotations\u001b[0m/\n",
            "cleaned_descriptions.json\n",
            "features_for_all_images_Flickr_backup.pkl\n",
            "features_for_all_images_Flickr_NEW.pkl\n",
            "features_for_all_images_Flickr.pkl\n",
            "features_for_all_images_Inception_MSCOCO.pkl\n",
            "\u001b[01;34mFlicker8k_Dataset\u001b[0m/\n",
            "\u001b[01;34mFlickr8k_text\u001b[0m/\n",
            "Image_captioning_inceptV3_Flickr_main.ipynb\n",
            "Image_captioning_inceptV3_MSCOCO.ipynb\n",
            "\u001b[01;34m__MACOSX\u001b[0m/\n",
            "model.png\n",
            "\u001b[01;34mMODELS\u001b[0m/\n",
            "tokenizer.pkl\n",
            "\u001b[01;34mtrain2014\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX5UBoHNIIEn"
      },
      "source": [
        "**Import Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43lfVoIaJaF_"
      },
      "source": [
        "import re\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import numpy             as np\n",
        "import tensorflow        as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tqdm          import tqdm\n",
        "from glob          import glob\n",
        "from PIL           import Image\n",
        "from numpy         import array\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from tensorflow.keras.models    import Model\n",
        "from tensorflow.keras.layers    import Input\n",
        "from tensorflow.keras.layers    import Dense\n",
        "from tensorflow.keras.layers    import LSTM\n",
        "from keras.optimizers           import Adam\n",
        "from keras.optimizers           import RMSprop\n",
        "from tensorflow.keras.layers    import Dropout\n",
        "from tensorflow.keras.layers    import Embedding\n",
        "from tensorflow.keras.utils     import plot_model\n",
        "from nltk.translate.bleu_score  import corpus_bleu\n",
        "from tensorflow.keras.utils     import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from sklearn.model_selection    import train_test_split\n",
        "\n",
        "from tensorflow.keras.preprocessing.text     import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iUabICDID2O"
      },
      "source": [
        "**Define Flags**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUYluZZwJaGc"
      },
      "source": [
        "PROGRESSIVE_LOADING = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRCvwAA2CFoV"
      },
      "source": [
        "**Load Cleaned Descriptions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ITcGe57tUzK"
      },
      "source": [
        "def load_desc_dict(filename):\n",
        "    print('\\nLoading cleaned descriptions file...')\n",
        "    # Load pickled file \n",
        "    with open(filename, 'rb') as f:\n",
        "      desc_dict = json.load(f)\n",
        "    print('There are {} images.'.format(len(desc_dict)))\n",
        "    return desc_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFI5gr-vrRLk",
        "outputId": "fab2f907-9bb8-4dbe-e2d7-71769fc792b9"
      },
      "source": [
        "ll"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 999441\n",
            "drwx------ 2 root      4096 Nov  4 22:05 \u001b[0m\u001b[01;34mannotations\u001b[0m/\n",
            "-rw------- 1 root   3047050 Nov 23 02:00 cleaned_descriptions.json\n",
            "-rw------- 1 root 133296868 Nov 18 18:35 features_for_all_images_Flickr_backup.pkl\n",
            "-rw------- 1 root  67015398 Nov 18 20:02 features_for_all_images_Flickr_NEW.pkl\n",
            "-rw------- 1 root 133296868 Nov 16 17:57 features_for_all_images_Flickr.pkl\n",
            "-rw------- 1 root 686187837 Nov  3 17:40 features_for_all_images_Inception_MSCOCO.pkl\n",
            "drwx------ 2 root      4096 Nov 17 01:49 \u001b[01;34mFlicker8k_Dataset\u001b[0m/\n",
            "drwx------ 2 root      4096 Nov 17 01:51 \u001b[01;34mFlickr8k_text\u001b[0m/\n",
            "-rw------- 1 root     54298 Nov 23 17:12 Image_captioning_inceptV3_Flickr_main.ipynb\n",
            "-rw------- 1 root     67040 Nov 15 19:57 Image_captioning_inceptV3_MSCOCO.ipynb\n",
            "drwx------ 2 root      4096 Nov 17 01:49 \u001b[01;34m__MACOSX\u001b[0m/\n",
            "-rw------- 1 root     55769 Nov 23 14:28 model.png\n",
            "drwx------ 2 root      4096 Nov 23 14:50 \u001b[01;34mMODELS\u001b[0m/\n",
            "-rw------- 1 root    379745 Nov 23 17:04 tokenizer.pkl\n",
            "drwx------ 2 root      4096 Nov  5 17:40 \u001b[01;34mtrain2014\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlAugIpyJaIM"
      },
      "source": [
        "**Load Images Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ31AIuyJaIO"
      },
      "source": [
        "def load_images_features(all_images_features_file_name,desc_dict_keys):\n",
        "    print('\\nLoading images features...')\n",
        "    # Load pickled file \n",
        "    with open(all_images_features_file_name, 'rb') as f:\n",
        "        all_images_features = pickle.load(f)\n",
        "    # Create images features dictionary\n",
        "    images_features = {key:all_images_features[key] for key in desc_dict_keys}\n",
        "    print('Images features loaded!')\n",
        "    single_image_features_shape = all_images_features[list(desc_dict_keys)[0]].shape\n",
        "    print('There are {} images.'.format(len(images_features)))\n",
        "    print('Single image features shape is {}'.format(single_image_features_shape))\n",
        "    return images_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhYS5EAJaId"
      },
      "source": [
        "**Split Data Into Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo6jajTjJaIg"
      },
      "source": [
        "def split_data(desc_dict, images_features, train_split_percent):\n",
        "    print('\\nSplitting data...')\n",
        "    # Split data into training and validation datasets\n",
        "    train_set_size = round(len(desc_dict)*train_split_percent)\n",
        "    valid_set_size = round(len(desc_dict)*(1-train_split_percent))\n",
        "    print('train_set_size: {}'.format(train_set_size))\n",
        "    print('valid_set_size: {}'.format(valid_set_size))\n",
        "    # If sized do not sum up, print a warning\n",
        "    if (train_set_size + valid_set_size == len(desc_dict)): pass\n",
        "    else: print('Warning: train and valid sets sizes do not sum up to the whole dataset size!!!')\n",
        "    # Shuffle images ids before splitting them\n",
        "    images_ids = list(desc_dict.keys())\n",
        "    random.shuffle(images_ids)\n",
        "    # Split images ids\n",
        "    train_images_ids = images_ids[:train_set_size]\n",
        "    valid_images_ids = images_ids[train_set_size:]\n",
        "    print('train_images_ids size :',len(train_images_ids))\n",
        "    print('valid_images_ids size :',len(valid_images_ids))\n",
        "    # Define empty lists\n",
        "    captions_train = dict()\n",
        "    captions_valid = dict()\n",
        "    images_features_train = dict()\n",
        "    images_features_valid = dict()\n",
        "    # Loop for train\n",
        "    for i in range(len(train_images_ids)):\n",
        "        key = train_images_ids[i]\n",
        "        if key not in captions_train.keys():\n",
        "            captions_train[key] = list()\n",
        "        captions_train[key].append(desc_dict[key])        \n",
        "        if key not in images_features_train.keys():\n",
        "            images_features_train[key] = list()\n",
        "        images_features_train[key].append(images_features[key])\n",
        "    # Loop for valid\n",
        "    for i in range(len(valid_images_ids)):\n",
        "        key = valid_images_ids[i]\n",
        "        if key not in captions_train.keys():\n",
        "            captions_valid[key] = list()\n",
        "        captions_valid[key].append(desc_dict[key])\n",
        "        if key not in images_features_valid.keys():\n",
        "            images_features_valid[key] = list()\n",
        "        images_features_valid[key].append(images_features[key])\n",
        "    # Print info\n",
        "    print('captions_train size       :', len(captions_train))\n",
        "    print('images_features_train size:', len(images_features_train))\n",
        "    print('captions_valid size       :', len(captions_valid))\n",
        "    print('images_features_valid size:', len(images_features_valid))\n",
        "    return captions_train, captions_valid, images_features_train, images_features_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyDbOz8SG5K8"
      },
      "source": [
        "**Create Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ_yE2GQJaIw"
      },
      "source": [
        "def create_tokenizer(captions_train):\n",
        "    # Convert captions_train from a dictionary to a list. This is needed for the following step.\n",
        "    captions_train_list = list()\n",
        "    captions_lengths = dict()\n",
        "    unique_words = set()\n",
        "    for key in captions_train.keys():\n",
        "        for caption in captions_train[key][0]:\n",
        "            captions_train_list.append(caption)\n",
        "            # Save captions lengths, to calcualte the max_length\n",
        "            if key not in captions_lengths.keys():\n",
        "                captions_lengths[key] = list()\n",
        "            captions_lengths[key].append(len(caption.split())-2)\n",
        "            # Update unique_words set\n",
        "            unique_words.update(caption.split())\n",
        "    num_uniqe_words = len(unique_words)\n",
        "    print('--------------------------')\n",
        "    print('There are {} unique words in the training dataset.'.format(num_uniqe_words))\n",
        "    key_max = max(captions_lengths, key=captions_lengths.get)\n",
        "    print(key_max)\n",
        "    max_length = max(captions_lengths[key_max])\n",
        "    print('The longest caption is {} words length.'.format(max_length))\n",
        "    # print(captions_train[key_max][0])\n",
        "    # Create tokenizer \n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_uniqe_words, oov_token='<unk>')\n",
        "    print('Tokenizer created!')\n",
        "    print('--------------------------')\n",
        "    # Fit tokenizer on training captions\n",
        "    tokenizer.fit_on_texts(captions_train_list)\n",
        "    # save the tokenizer. it will be needed later once the model is trained and loaded for testing.\n",
        "#     print('Saving tokenizer to \\'tokenizer.pkl\\'')\n",
        "    with open('tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    return tokenizer, num_uniqe_words, max_length\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsDBOM9DHUPG"
      },
      "source": [
        "**Create Sequences**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Y_dDpsq3Xc"
      },
      "source": [
        "def create_sequences(captions, images_features, tokenizer, num_uniqe_words, max_length):\n",
        "    # Define X1 (for image features), X2 (for input sequence), and y (for output sequence).\n",
        "    X1 = list()\n",
        "    X2 = list()\n",
        "    y  = list()\n",
        "    # Pad input sequences and one-hot-encode output sequences\n",
        "    # for captions\n",
        "    for key, captions_list in tqdm(captions.items(), desc='Creating sequence'):\n",
        "        for caption in captions_list[0]:\n",
        "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "            for i in range(1,len(seq)):\n",
        "                in_seq  = seq[:i]\n",
        "                out_seq = seq[i]\n",
        "                # Padding\n",
        "                in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                out_seq = tf.keras.utils.to_categorical(y=[out_seq], num_classes=num_uniqe_words)[0]\n",
        "                X1.append(images_features[key][0][0])\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    # Make them arrays\n",
        "    X1 = array(X1) \n",
        "    X2 = array(X2, dtype='uint16') \n",
        "    y  = array(y, dtype='uint16')\n",
        "    # print('X1.shape:',X1.shape)\n",
        "    # print('X2.shape:',X2.shape)\n",
        "    # print('y.shape :',y.shape)\n",
        "    # print()\n",
        "    return X1, X2, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9NDieYGH4ox"
      },
      "source": [
        "**Data Generator (if needed)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys9ZixG41rm5"
      },
      "source": [
        "def data_generator(captions, images_features, tokenizer,\n",
        "                   num_uniqe_words, max_length, num_images_generated):\n",
        "    # Define X1 (for image features), X2 (for input sequence), and y (for output sequence).\n",
        "    X1 = list()\n",
        "    X2 = list()\n",
        "    y  = list()\n",
        "    counter = 0\n",
        "    # Pad input sequences and one-hot-encode output sequences\n",
        "    # for captions\n",
        "    while 1:\n",
        "      for key, captions_list in captions.items():\n",
        "        # print(key)\n",
        "        counter+=1\n",
        "        # print(counter)\n",
        "        for caption in captions_list[0]:\n",
        "          seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "          for i in range(1,len(seq)):\n",
        "            in_seq  = seq[:i]\n",
        "            out_seq = seq[i]\n",
        "            # Padding\n",
        "            in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = tf.keras.utils.to_categorical(y=[out_seq], num_classes=num_uniqe_words)[0]\n",
        "            X1.append(images_features[key][0][0])\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "        if counter == num_images_generated:\n",
        "        # Make them arrays\n",
        "          # print('Number of required data generated!')\n",
        "          X1 = array(X1) \n",
        "          X2 = array(X2, dtype='uint16') \n",
        "          y  = array(y, dtype='uint16')\n",
        "          yield ([X1, X2], y)\n",
        "          X1 = list()\n",
        "          X2 = list()\n",
        "          y  = list()\n",
        "          counter = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGGrqvqz7q8s"
      },
      "source": [
        "# ?model.compile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLeZaEDPNUMA"
      },
      "source": [
        "----------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1v-GSOUHBth"
      },
      "source": [
        "**Main**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKBOmIxVtKju",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8fee5d5-e243-4f7a-892a-ace4a43bef1c"
      },
      "source": [
        "# Parse descriptions data\n",
        "desc_dict = load_desc_dict('cleaned_descriptions.json')\n",
        "\n",
        "# Load images features\n",
        "filename_images_features = 'features_for_all_images_Flickr_NEW.pkl'\n",
        "images_features = load_images_features(filename_images_features,desc_dict.keys())\n",
        "\n",
        "# Split data \n",
        "captions_train, captions_valid, \\\n",
        "images_features_train, images_features_valid = split_data(desc_dict, \n",
        "                                                          images_features, \n",
        "                                                          train_split_percent=0.85)\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer, num_uniqe_words, max_length = create_tokenizer(captions_train)\n",
        "\n",
        "# Get the vocabs size\n",
        "vocabs_size = num_uniqe_words\n",
        "\n",
        "if PROGRESSIVE_LOADING == 0:\n",
        "  # Create training sequence\n",
        "  X1_train, X2_train, y_train = create_sequences(captions_train,\n",
        "                                                 images_features_train,\n",
        "                                                 tokenizer,\n",
        "                                                 num_uniqe_words,\n",
        "                                                 max_length)\n",
        "  print()\n",
        "  print()\n",
        "  print('X1_train.shape:',X1_train.shape)\n",
        "  print('X2_train.shape:',X2_train.shape)\n",
        "  print('y_train.shape :',y_train.shape)\n",
        "\n",
        "  # Create validation sequence\n",
        "  X1_valid, X2_valid, y_valid = create_sequences(captions_valid,\n",
        "                                                 images_features_valid,\n",
        "                                                 tokenizer,\n",
        "                                                 num_uniqe_words,\n",
        "                                                 max_length)\n",
        "  print()\n",
        "  print('X1_valid.shape:',X1_valid.shape)\n",
        "  print('X2_valid.shape:',X2_valid.shape)\n",
        "  print('y_valid.shape :',y_valid.shape)\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading cleaned descriptions file...\n",
            "There are 8091 images.\n",
            "\n",
            "Loading images features...\n",
            "Images features loaded!\n",
            "There are 8091 images.\n",
            "Single image features shape is (1, 2048)\n",
            "\n",
            "Splitting data...\n",
            "train_set_size: 6877\n",
            "valid_set_size: 1214\n",
            "train_images_ids size : 6877\n",
            "valid_images_ids size : 1214\n",
            "captions_train size       : 6877\n",
            "images_features_train size: 6877\n",
            "captions_valid size       : 1214\n",
            "images_features_valid size: 1214\n",
            "--------------------------\n",
            "There are 8157 unique words in the training dataset.\n",
            "1499495021_d295ce577c\n",
            "The longest caption is 28 words length.\n",
            "Tokenizer created!\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating sequence: 100%|██████████| 6877/6877 [00:16<00:00, 426.47it/s]\n",
            "Creating sequence:   5%|▍         | 57/1214 [00:00<00:02, 565.44it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "X1_train.shape: (351315, 2048)\n",
            "X2_train.shape: (351315, 28)\n",
            "y_train.shape : (351315, 8157)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Creating sequence: 100%|██████████| 1214/1214 [00:02<00:00, 516.18it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "X1_valid.shape: (62039, 2048)\n",
            "X2_valid.shape: (62039, 28)\n",
            "y_valid.shape : (62039, 8157)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84gNp21tHuE6"
      },
      "source": [
        "**Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvXu-IT0JaJL"
      },
      "source": [
        "# from tensorflow.keras.layers.merge import add\n",
        "# from tensorflow.keras.layers.merge import concatenate\n",
        "# '''As of keras 2, the module keras.layers.merge doesn't have a generic public Merge-Layer. \n",
        "# Instead you are supposed to import the subclasses like keras.layers.Add or keras.layers.Concatenate etc. \n",
        "# directly (or their functional interfaces with the same names lowercase: keras.layers.add, \n",
        "# keras.layers.concatenate etc.). See what types of merging layers exist in the keras docs'''\n",
        "\n",
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length, LR, dropout, num_neurons):\n",
        "    # num_neurons = 2**9  # 2**9 = 512\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(dropout)(inputs1)\n",
        "    fe2 = Dense(num_neurons, activation='relu')(fe1)\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, num_neurons, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(dropout)(se1)\n",
        "    se3 = LSTM(num_neurons)(se2)\n",
        "    # decoder model\n",
        "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
        "    decoder2 = Dense(num_neurons, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=RMSprop(lr=LR))# RMSprop  Adam\n",
        "    # summarize model\n",
        "#     print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLiMNKUKHoch"
      },
      "source": [
        "**Model Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEXZAo33KIiD"
      },
      "source": [
        "With No Progressive Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTiZzECjUAy-",
        "outputId": "977c6e37-d182-4ac4-be0d-569a327a511d"
      },
      "source": [
        "if PROGRESSIVE_LOADING == 0:  \n",
        "  epochs      = 20\n",
        "  batch_size  = 2**12  #2^5 = 32\n",
        "  num_neurons = 2**10  # 2**9 = 512\n",
        "  LR          = 0.001\n",
        "  dropout     = 0.8\n",
        "  print('vocabs_size:', vocabs_size)\n",
        "  print('max_length :', max_length)\n",
        "  print()\n",
        "  print('epochs     :', epochs)\n",
        "  print('batch size :', batch_size)\n",
        "  print('LR         :', LR)\n",
        "  print('dropout    :', dropout)\n",
        "  print('num_neurons:', num_neurons)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabs_size: 8157\n",
            "max_length : 28\n",
            "\n",
            "epochs     : 20\n",
            "batch size : 4096\n",
            "LR         : 0.001\n",
            "dropout    : 0.8\n",
            "num_neurons: 1024\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kdTnjQwhbJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a607f86-bc0a-4d0f-d8e0-139777ae7e57"
      },
      "source": [
        "if PROGRESSIVE_LOADING == 0:\n",
        "  print('vocabs_size:', vocabs_size)\n",
        "  print('max_length :', max_length)\n",
        "  print()\n",
        "  print('epochs     :', epochs)\n",
        "  print('batch size :', batch_size)\n",
        "  print('LR         :', LR)\n",
        "  print('dropout    :', dropout)\n",
        "  print('num_neurons:', num_neurons)\n",
        "  print()\n",
        "  # Define the model\n",
        "  model = define_model(vocabs_size, max_length, LR, dropout, num_neurons)\n",
        "  # define checkpoint callback\n",
        "  filepath = 'MODELS/' + 'model-' + 'batch_size' + str(batch_size) + \\\n",
        "    '-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "  # filepath = 'MODELS/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  # fit model\n",
        "  history = model.fit(x=[X1_train, X2_train],\n",
        "                      y=y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs, \n",
        "                      verbose=1, \n",
        "                      callbacks=[checkpoint], \n",
        "                      validation_data=([X1_valid, X2_valid], y_valid))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocabs_size: 8157\n",
            "max_length : 28\n",
            "\n",
            "epochs     : 20\n",
            "batch size : 4096\n",
            "LR         : 0.001\n",
            "dropout    : 0.8\n",
            "num_neurons: 1024\n",
            "\n",
            "Epoch 1/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 5.5705\n",
            "Epoch 00001: val_loss improved from inf to 4.42974, saving model to MODELS/model-batch_size4096-ep001-loss5.571-val_loss4.430.h5\n",
            "86/86 [==============================] - 74s 862ms/step - loss: 5.5705 - val_loss: 4.4297\n",
            "Epoch 2/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 4.1483\n",
            "Epoch 00002: val_loss improved from 4.42974 to 3.91188, saving model to MODELS/model-batch_size4096-ep002-loss4.148-val_loss3.912.h5\n",
            "86/86 [==============================] - 73s 847ms/step - loss: 4.1483 - val_loss: 3.9119\n",
            "Epoch 3/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 3.7060\n",
            "Epoch 00003: val_loss improved from 3.91188 to 3.77080, saving model to MODELS/model-batch_size4096-ep003-loss3.706-val_loss3.771.h5\n",
            "86/86 [==============================] - 73s 849ms/step - loss: 3.7060 - val_loss: 3.7708\n",
            "Epoch 4/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 3.4435\n",
            "Epoch 00004: val_loss improved from 3.77080 to 3.56066, saving model to MODELS/model-batch_size4096-ep004-loss3.443-val_loss3.561.h5\n",
            "86/86 [==============================] - 73s 848ms/step - loss: 3.4435 - val_loss: 3.5607\n",
            "Epoch 5/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 3.2516\n",
            "Epoch 00005: val_loss did not improve from 3.56066\n",
            "86/86 [==============================] - 72s 836ms/step - loss: 3.2516 - val_loss: 3.5965\n",
            "Epoch 6/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 3.0869\n",
            "Epoch 00006: val_loss improved from 3.56066 to 3.49089, saving model to MODELS/model-batch_size4096-ep006-loss3.087-val_loss3.491.h5\n",
            "86/86 [==============================] - 73s 844ms/step - loss: 3.0869 - val_loss: 3.4909\n",
            "Epoch 7/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.9475\n",
            "Epoch 00007: val_loss improved from 3.49089 to 3.47566, saving model to MODELS/model-batch_size4096-ep007-loss2.948-val_loss3.476.h5\n",
            "86/86 [==============================] - 73s 844ms/step - loss: 2.9475 - val_loss: 3.4757\n",
            "Epoch 8/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.8155\n",
            "Epoch 00008: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 72s 833ms/step - loss: 2.8155 - val_loss: 3.6349\n",
            "Epoch 9/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.6914\n",
            "Epoch 00009: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 72s 833ms/step - loss: 2.6914 - val_loss: 3.6328\n",
            "Epoch 10/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.5722\n",
            "Epoch 00010: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 830ms/step - loss: 2.5722 - val_loss: 3.7671\n",
            "Epoch 11/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.4562\n",
            "Epoch 00011: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 830ms/step - loss: 2.4562 - val_loss: 3.8368\n",
            "Epoch 12/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.3464\n",
            "Epoch 00012: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 829ms/step - loss: 2.3464 - val_loss: 3.8860\n",
            "Epoch 13/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.2422\n",
            "Epoch 00013: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 830ms/step - loss: 2.2422 - val_loss: 3.9118\n",
            "Epoch 14/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.1447\n",
            "Epoch 00014: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 829ms/step - loss: 2.1447 - val_loss: 4.1240\n",
            "Epoch 15/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 2.0509\n",
            "Epoch 00015: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 829ms/step - loss: 2.0509 - val_loss: 3.9528\n",
            "Epoch 16/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.9682\n",
            "Epoch 00016: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 830ms/step - loss: 1.9682 - val_loss: 4.0609\n",
            "Epoch 17/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.8838\n",
            "Epoch 00017: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 831ms/step - loss: 1.8838 - val_loss: 4.4256\n",
            "Epoch 18/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.8122\n",
            "Epoch 00018: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 72s 832ms/step - loss: 1.8122 - val_loss: 4.5142\n",
            "Epoch 19/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.7393\n",
            "Epoch 00019: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 829ms/step - loss: 1.7393 - val_loss: 4.6912\n",
            "Epoch 20/20\n",
            "86/86 [==============================] - ETA: 0s - loss: 1.6774\n",
            "Epoch 00020: val_loss did not improve from 3.47566\n",
            "86/86 [==============================] - 71s 827ms/step - loss: 1.6774 - val_loss: 4.5321\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wxzUC_NJaNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "outputId": "b7456e93-7e1d-44af-a9d1-b2f2f3ce4fc9"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.yscale('log')\n",
        "# plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'valid'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'val_loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLhgrURnKblz"
      },
      "source": [
        "With Progressive Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvQVQNCJaNV"
      },
      "source": [
        "# batch size = 2,4,8,16, 32,64, 128,256,512,1024,2048,4096,8192,16384 (2^n)\n",
        "if PROGRESSIVE_LOADING == 1:\n",
        "  # Define model\n",
        "  model = define_model(vocabs_size, max_length)\n",
        "  # Define fitting hyperparameters\n",
        "  epochs = 20\n",
        "  # batch_size_train = 100\n",
        "  # batch_size_valid = 100\n",
        "  num_images_per_batch_train = 7\n",
        "  num_images_per_batch_valid = 7\n",
        "  batch_size_train = math.ceil(len(captions_train)/num_images_per_batch_train)\n",
        "  batch_size_valid = math.ceil(len(captions_valid)/num_images_per_batch_valid)\n",
        "  # steps_per_epochs_train = num_images_per_batch_train\n",
        "  # steps_per_epochs_valid = num_images_per_batch_valid\n",
        "  print('Num. of training images  :',len(captions_train))\n",
        "  print('Num. of validation images:',len(captions_valid))\n",
        "  print('Num. of training images per batch  :',num_images_per_batch_train)\n",
        "  print('Num. of validation images per batch:',num_images_per_batch_valid)\n",
        "  print('Batch size train:',batch_size_train)\n",
        "  print('Batch size valid:',batch_size_valid)\n",
        "  print('Num. of epochs:', epochs)\n",
        "  # print('steps_per_epochs_train:',steps_per_epochs_train)\n",
        "  # print('steps_per_epochs_valid:',steps_per_epochs_valid)\n",
        "  # Loop for fitting\n",
        "  for i in range(1,epochs):\n",
        "    print('Epoch #',i)\n",
        "    # Create the data generator\n",
        "    generator_train = data_generator(captions_train, images_features_train,\n",
        "                                     tokenizer, num_uniqe_words,\n",
        "                                     max_length, num_images_per_batch_train)\n",
        "    \n",
        "    generator_valid = data_generator(captions_valid, images_features_valid,\n",
        "                                     tokenizer, num_uniqe_words,\n",
        "                                     max_length, num_images_per_batch_valid)\n",
        "    # Fit for one epoch\n",
        "    model.fit(x=generator_train, epochs=1, verbose=1,\n",
        "              steps_per_epoch=batch_size_train,\n",
        "              validation_data=generator_valid,\n",
        "              validation_steps=batch_size_valid,\n",
        "              max_queue_size=1, workers=1,\n",
        "              use_multiprocessing=False)\n",
        "    \n",
        "    # model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    # Save model\n",
        "    model.save('MODELS/model_BS'+ str(batch_size_train) + '_EP' + str(i) + '.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCbg_jkRIuoi"
      },
      "source": [
        "**Finally, Model Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPoEBxNlJaNs"
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, image_features, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image_features, sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc82YmoZJaOH"
      },
      "source": [
        "# evaluate the skill of the model\n",
        "def evaluate_model(model, descriptions, images_features, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # generate description\n",
        "        yhat = generate_desc(model, tokenizer, images_features[key][0], max_length)\n",
        "        # store actual and predicted\n",
        "        references = [d.split() for d in desc_list[0]]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilYvmWa7JaOR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6001c006-c7ea-483c-ce47-4ba8f53cfd0d"
      },
      "source": [
        "from keras.models import load_model\n",
        "# load the model\n",
        "# filename = 'MODELS/model-batch_size16384-ep020-loss4.922-val_loss5.565.h5'\n",
        "# model = load_model(filename)\n",
        "evaluate_model(model, captions_valid, images_features_valid, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU-1: 0.132812\n",
            "BLEU-2: 0.033268\n",
            "BLEU-3: 0.129783\n",
            "BLEU-4: 0.182396\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-BStszJaOY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}