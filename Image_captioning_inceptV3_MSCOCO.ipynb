{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "Image_captioning_inceptV3_MSCOCO.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anas321/ImageCaptioning/blob/master/Image_captioning_inceptV3_MSCOCO.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zO246bz9iDcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9882a1c-0c2e-40f8-d1da-9cf755e1b419"
      },
      "source": [
        "!pip install pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.10.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.18.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYk_0t8kNSUa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "beda5eaf-afe8-41ba-98e2-02fe8a1cd50a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFh_KR9PNhzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a58ca7-2f0e-4f70-9acb-be367498e274"
      },
      "source": [
        "cd /content/drive/My Drive/Colab Notebooks/ImageCaptioning"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ImageCaptioning\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_EFMAmSgSjkD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "6e23c3e1-bcd6-40e6-e834-06da562ffd1f"
      },
      "source": [
        "pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/Colab Notebooks/ImageCaptioning'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2JR6VIBSmPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "856c675d-1687-4144-dd8f-5258196ab118"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mannotations\u001b[0m/                                  model.png\n",
            "cleaned_descriptions.json                     \u001b[01;34mMODELS\u001b[0m/\n",
            "features_for_all_images_Inception_MSCOCO.pkl  tokenizer.pkl\n",
            "\u001b[01;34mFlickr\u001b[0m/                                       \u001b[01;34mtrain2014\u001b[0m/\n",
            "Image_captioning_inceptV3_MSCOCO.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUYluZZwJaGc"
      },
      "source": [
        "EXTRACT_IMAGES_FEATURES = 0\n",
        "PROGRESSIVE_LOADING     = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43lfVoIaJaF_"
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "# import collections\n",
        "import random\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import json\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import pickle\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ud0sSVpNJaG1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74755219-2a35-45ae-f377-dd4ddb724501"
      },
      "source": [
        "# Download annotations files\n",
        "annotations_url = 'http://images.cocodataset.org/annotations/annotations_trainval2014.zip'\n",
        "annotations_folder_name = 'annotations'\n",
        "\n",
        "if os.path.exists(os.path.abspath('') + '/' + annotations_folder_name):\n",
        "  annotations_folder_path = os.path.abspath('') + '/' + annotations_folder_name\n",
        "  print('annotations folder found!')\n",
        "else:\n",
        "  print('annotations file does not exit. Downloading it from source now...')\n",
        "  annotations_zipped = tf.keras.utils.get_file(fname='annotations.zip',\n",
        "                                                origin=annotations_url,\n",
        "                                                extract=True)\n",
        "  os.remove(annotations_zipped)\n",
        "  annotations_folder_path = os.path.abspath('') + '/' + annotations_folder_name\n",
        "\n",
        "annotations_file_path = os.path.abspath(annotations_folder_path) + '/captions_train2014.json' \n",
        "#annotations_file_train = os.path.abspath(annotations_folder) + '/annotations/captions_train2014.json'\n",
        "print('annotations_file_path is:', annotations_file_path)\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "annotations folder found!\n",
            "annotations_file_path is: /content/drive/My Drive/Colab Notebooks/ImageCaptioning/annotations/captions_train2014.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuTJU3erJaHN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bca151cf-8bcd-4ddf-a431-9d1ddde2ea4d"
      },
      "source": [
        "# Download images\n",
        "images_url = 'http://images.cocodataset.org/zips/train2014.zip'\n",
        "images_folder_name = 'train2014'\n",
        "\n",
        "print(os.path.abspath('') + '/' + images_folder_name)\n",
        "\n",
        "if os.path.exists(os.path.abspath('') + '/' + images_folder_name):\n",
        "  images_folder_train_path = os.path.abspath('') + '/' + images_folder_name\n",
        "  print('images folder found!')\n",
        "  print('images_folder_train_path is:', images_folder_train_path)\n",
        "else:\n",
        "  # print('images folder does not exit. Downloading it from source now (warning: 13 GB)...')\n",
        "  # images_zipped = tf.keras.utils.get_file(fname='train2014.zip',\n",
        "  #                                               origin=images_url,\n",
        "  #                                               extract=True)\n",
        "  !wget 'http://images.cocodataset.org/zips/train2014.zip'\n",
        "  ! unzip train2014.zip\n",
        "  ! rm train2014.zip\n",
        "# os.remove(images_zipped)\n",
        "images_folder_train_path = os.path.abspath('') + '/' + images_folder_name"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/ImageCaptioning/train2014\n",
            "images folder found!\n",
            "images_folder_train_path is: /content/drive/My Drive/Colab Notebooks/ImageCaptioning/train2014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEv1oU20JaHe"
      },
      "source": [
        "**Extract Images Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFXct8ZaJaHi"
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.models import Model\n",
        "from tqdm import tqdm\n",
        "\n",
        "images_folder_train = 'train2014'\n",
        "all_images_features_file_name = 'features_for_all_images_Inception_MSCOCO.pkl'\n",
        "\n",
        "# extract features from each image\n",
        "def extract_images_features(directory):\n",
        "    # load the VGG16 model\n",
        "    model = InceptionV3()\n",
        "    # re-structure the model\n",
        "    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n",
        "    # print model summary\n",
        "    print(model.summary())\n",
        "    # define all_features dictionery to store \n",
        "    # features for all images\n",
        "    all_features = dict()\n",
        "    #counter = 1\n",
        "    for name in tqdm(listdir(directory)):\n",
        "        # get an image from a file\n",
        "        filename = directory + '/' + name\n",
        "        #print(filename)\n",
        "        image = load_img(filename, target_size=(299,299))\n",
        "        # convert the image pixles to a numpy array\n",
        "        image = img_to_array(image)\n",
        "        #print(image.shape)\n",
        "        # reshape the created numpy array for the model\n",
        "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
        "        #print(image.shape)\n",
        "        # prepare the image for the VGG16 model\n",
        "        image = preprocess_input(image)\n",
        "        # get features\n",
        "        feature = model.predict(image, verbose=0)\n",
        "        # get image id\n",
        "        image_name = name.split('.')[0]\n",
        "        #print(image_name)\n",
        "        # store feature in the features dictionery\n",
        "        all_features[image_name] = feature\n",
        "        #print(counter,'>', name)\n",
        "        #counter += 1\n",
        "    return all_features\n",
        "\n",
        "if EXTRACT_IMAGES_FEATURES == 1:\n",
        "    # extract features from all images\n",
        "    directory = images_folder_train\n",
        "    features = extract_images_features(directory)\n",
        "    print('Size of extracted features:', len(features))\n",
        "    # save to file\n",
        "    with open(all_images_features_file_name, 'wb') as f:\n",
        "        pickle.dump(features, f)\n",
        "#     dump(features, open('features_for_all_images_Inception_MSCOCO.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZJPVKsvJaH2"
      },
      "source": [
        "**Parse Annotations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qegjDwhBJaH6"
      },
      "source": [
        "import string\n",
        "def parse_annotations(annotations_file_path, percentage_of_selected_images):\n",
        "    # Load annotations json file\n",
        "    with open(annotations_file_path, 'r') as f:\n",
        "        file_content = json.load(f)\n",
        "    # Create a dictionary where keys are the images ids and values are the captions\n",
        "    desc_dict = dict()\n",
        "    for val in file_content['annotations']:\n",
        "        # Example of image name: COCO_train2014_000000282782.jpg\n",
        "        image_id = 'COCO_train2014_' + '%012d' %(val['image_id']) #+ '.jpg'\n",
        "        caption  = 'startseq ' + val['caption'] + ' endseq'\n",
        "        if image_id not in desc_dict.keys():\n",
        "            desc_dict[image_id] = list()\n",
        "        desc_dict[image_id].append(caption)\n",
        "    # select only percentage of the available images (for memory restrictions)\n",
        "    if percentage_of_selected_images < 1.0:\n",
        "        num_of_selected_images = round(len(desc_dict.keys())*percentage_of_selected_images)\n",
        "        desc_dict = list(desc_dict.items())[0:num_of_selected_images]\n",
        "        desc_dict = dict(desc_dict)\n",
        "    # Loop through the desc_dict\n",
        "    for key, desc_list in desc_dict.items():\n",
        "        # Loop through the list for each key\n",
        "        for i in range(len(desc_list)):\n",
        "            current_desc = desc_list[i]\n",
        "            # Tokenize; i.e., split\n",
        "            current_desc = current_desc.split()\n",
        "            # Lower case all words\n",
        "            current_desc = [word.lower() for word in current_desc]\n",
        "            # Remove any words with length <= 1\n",
        "            current_desc = [word for word in current_desc if len(word)>1]\n",
        "            # Remove any puctuations\n",
        "            for j in range(len(current_desc)):\n",
        "                current_word = current_desc[j]\n",
        "                if not current_word.isalnum(): \n",
        "                    letters = list(current_word)\n",
        "#                     print(letters)\n",
        "                    letters = [letter for letter in letters if letter not in string.punctuation]\n",
        "                    current_desc[j] = ''.join(letters)\n",
        "#                     print(current_desc[j])\n",
        "            # Remove any words happen to contain numbers in them\n",
        "            current_desc = [word for word in current_desc if word.isalpha()]\n",
        "            # Join the splitted words together again\n",
        "            current_desc = ' '.join(current_desc)\n",
        "            # Assain it back to the dictionery for its corresponding key\n",
        "            desc_list[i] = current_desc\n",
        "    # Print length of desc_dict\n",
        "    print('There are {:d} images.'.format(len(desc_dict)))\n",
        "    # Save the cleaned descriptions as a json file\n",
        "    with open('cleaned_descriptions.json', 'w') as f:\n",
        "        json.dump(desc_dict, f)\n",
        "    print('\\nCleaned descriptions are saved in \\'cleaned_descriptions.json\\' file.')\n",
        "    \n",
        "    return desc_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlAugIpyJaIM"
      },
      "source": [
        "**Loading Images Features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZ31AIuyJaIO"
      },
      "source": [
        "def load_images_features(annotations_keys):\n",
        "    print('\\nLoading images features...')\n",
        "    # Load pickled file \n",
        "    with open(all_images_features_file_name, 'rb') as f:\n",
        "        all_images_features = pickle.load(f)\n",
        "    # Create images features dictionary\n",
        "    images_features = {key:all_images_features[key] for key in annotations_keys}\n",
        "    print('Images features loaded!')\n",
        "    single_image_features_shape = all_images_features[list(annotations_keys)[0]].shape\n",
        "    print('Single image features shape is {}'.format(single_image_features_shape))\n",
        "    return images_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhYS5EAJaId"
      },
      "source": [
        "**Split Data Into Training and Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jo6jajTjJaIg"
      },
      "source": [
        "def split_data(annotations, images_features, train_split_percent):\n",
        "    print('\\nSplitting data...')\n",
        "    # Split data into training and validation datasets\n",
        "    train_set_size = round(len(annotations)*train_split_percent)\n",
        "    valid_set_size = round(len(annotations)*(1-train_split_percent))\n",
        "    print('train_set_size: {}'.format(train_set_size))\n",
        "    print('valid_set_size: {}'.format(valid_set_size))\n",
        "    # If sized do not sum up, print a warning\n",
        "    if (train_set_size + valid_set_size == len(annotations)): pass\n",
        "    else: print('Warning: train and valid sets sizes do not sum up to the whole dataset size!!!')\n",
        "    # Shuffle images ids before splitting them\n",
        "    images_ids = list(annotations.keys())\n",
        "    random.shuffle(images_ids)\n",
        "    # Split images ids\n",
        "    train_images_ids = images_ids[:train_set_size]\n",
        "    valid_images_ids = images_ids[train_set_size:]\n",
        "    print('train_images_ids size :',len(train_images_ids))\n",
        "    print('valid_images_ids size :',len(valid_images_ids))\n",
        "    # Define empty lists\n",
        "    captions_train = dict()\n",
        "    captions_valid = dict()\n",
        "    images_features_train = dict()\n",
        "    images_features_valid = dict()\n",
        "    # Loop for train\n",
        "    for i in range(len(train_images_ids)):\n",
        "        key = train_images_ids[i]\n",
        "        if key not in captions_train.keys():\n",
        "            captions_train[key] = list()\n",
        "        captions_train[key].append(annotations[key])        \n",
        "        if key not in images_features_train.keys():\n",
        "            images_features_train[key] = list()\n",
        "        images_features_train[key].append(images_features[key])\n",
        "    # Loop for valid\n",
        "    for i in range(len(valid_images_ids)):\n",
        "        key = valid_images_ids[i]\n",
        "        if key not in captions_train.keys():\n",
        "            captions_valid[key] = list()\n",
        "        captions_valid[key].append(annotations[key])\n",
        "        if key not in images_features_valid.keys():\n",
        "            images_features_valid[key] = list()\n",
        "        images_features_valid[key].append(images_features[key])\n",
        "    # Print info\n",
        "    print('captions_train size       :', len(captions_train))\n",
        "    print('images_features_train size:', len(images_features_train))\n",
        "    print('captions_valid size       :', len(captions_valid))\n",
        "    print('images_features_valid size:', len(images_features_valid))\n",
        "    return captions_train, captions_valid, images_features_train, images_features_valid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ_yE2GQJaIw"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "def create_tokenizer(captions_train):\n",
        "    # Convert captions_train from a dictionary to a list. This is needed for the following step.\n",
        "    captions_train_list = list()\n",
        "    captions_lengths = dict()\n",
        "    unique_words = set()\n",
        "    for key in captions_train.keys():\n",
        "        for caption in captions_train[key][0]:\n",
        "            captions_train_list.append(caption)\n",
        "            # Save captions lengths, to calcualte the max_length\n",
        "            if key not in captions_lengths.keys():\n",
        "                captions_lengths[key] = list()\n",
        "            captions_lengths[key].append(len(caption.split())-2)\n",
        "            # Update unique_words set\n",
        "            unique_words.update(caption.split())\n",
        "    num_uniqe_words = len(unique_words)\n",
        "    print('--------------------------')\n",
        "    print('There are {} unique words in the training dataset.'.format(num_uniqe_words))\n",
        "    key_max = max(captions_lengths, key=captions_lengths.get)\n",
        "    print(key_max)\n",
        "    max_length = max(captions_lengths[key_max])\n",
        "    print('The longest caption is {} words length.'.format(max_length))\n",
        "    # print(captions_train[key_max][0])\n",
        "    # Create tokenizer \n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_uniqe_words, oov_token='<unk>')\n",
        "    print('Tokenizer created!')\n",
        "    print('--------------------------')\n",
        "    # Fit tokenizer on training captions\n",
        "    tokenizer.fit_on_texts(captions_train_list)\n",
        "    # save the tokenizer. it will be needed later once the model is trained and loaded for testing.\n",
        "#     print('Saving tokenizer to \\'tokenizer.pkl\\'')\n",
        "    with open('tokenizer.pkl', 'wb') as f:\n",
        "        pickle.dump(tokenizer, f)\n",
        "    return tokenizer, num_uniqe_words, max_length\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_Y_dDpsq3Xc"
      },
      "source": [
        "from numpy import array\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "def create_sequences(captions, images_features, tokenizer, num_uniqe_words, max_length):\n",
        "    # Define X1 (for image features), X2 (for input sequence), and y (for output sequence).\n",
        "    X1 = list()\n",
        "    X2 = list()\n",
        "    y  = list()\n",
        "    # Pad input sequences and one-hot-encode output sequences\n",
        "    # for captions\n",
        "    for key, captions_list in tqdm(captions.items(), desc='Creating training sequence'):\n",
        "        for caption in captions_list[0]:\n",
        "            seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "            for i in range(1,len(seq)):\n",
        "                in_seq  = seq[:i]\n",
        "                out_seq = seq[i]\n",
        "                # Padding\n",
        "                in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                out_seq = tf.keras.utils.to_categorical(y=[out_seq], num_classes=num_uniqe_words)[0]\n",
        "                X1.append(images_features[key][0][0])\n",
        "                X2.append(in_seq)\n",
        "                y.append(out_seq)\n",
        "    # Make them arrays\n",
        "    X1 = array(X1) \n",
        "    X2 = array(X2, dtype='uint16') \n",
        "    y  = array(y, dtype='uint16')\n",
        "    # print('X1.shape:',X1.shape)\n",
        "    # print('X2.shape:',X2.shape)\n",
        "    # print('y.shape :',y.shape)\n",
        "    # print()\n",
        "    return X1, X2, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ys9ZixG41rm5"
      },
      "source": [
        "def data_generator(captions, images_features, tokenizer,\n",
        "                   num_uniqe_words, max_length, num_images_generated):\n",
        "    # Define X1 (for image features), X2 (for input sequence), and y (for output sequence).\n",
        "    X1 = list()\n",
        "    X2 = list()\n",
        "    y  = list()\n",
        "    counter = 0\n",
        "    # Pad input sequences and one-hot-encode output sequences\n",
        "    # for captions\n",
        "    while 1:\n",
        "      for key, captions_list in captions.items():\n",
        "        # print(key)\n",
        "        counter+=1\n",
        "        # print(counter)\n",
        "        for caption in captions_list[0]:\n",
        "          seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "          for i in range(1,len(seq)):\n",
        "            in_seq  = seq[:i]\n",
        "            out_seq = seq[i]\n",
        "            # Padding\n",
        "            in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "            out_seq = tf.keras.utils.to_categorical(y=[out_seq], num_classes=num_uniqe_words)[0]\n",
        "            X1.append(images_features[key][0][0])\n",
        "            X2.append(in_seq)\n",
        "            y.append(out_seq)\n",
        "        if counter == num_images_generated:\n",
        "        # Make them arrays\n",
        "          # print('Number of required data generated!')\n",
        "          X1 = array(X1) \n",
        "          X2 = array(X2, dtype='uint16') \n",
        "          y  = array(y, dtype='uint16')\n",
        "          yield ([X1, X2], y)\n",
        "          X1 = list()\n",
        "          X2 = list()\n",
        "          y  = list()\n",
        "          counter = 0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGNrwonBzBlI"
      },
      "source": [
        "# def create_sequences_generator(captions_list, images_features, \n",
        "#                                tokenizer, num_uniqe_words, max_length):\n",
        "#     # Define X1 (for image features), X2 (for input sequence), and y (for output sequence).\n",
        "#     X1 = list()\n",
        "#     X2 = list()\n",
        "#     y  = list()\n",
        "#     # Pad input sequences and one-hot-encode output sequences\n",
        "#     # for captions\n",
        "#     # for key, captions_list in tqdm(captions.items(), desc='Creating training sequence'):\n",
        "#     for caption in captions_list[0]:\n",
        "#         seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "#         for i in range(1,len(seq)):\n",
        "#             in_seq  = seq[:i]\n",
        "#             out_seq = seq[i]\n",
        "#             # Padding\n",
        "#             in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "#             out_seq = tf.keras.utils.to_categorical(y=[out_seq], num_classes=num_uniqe_words)[0]\n",
        "#             X1.append(images_features)#[key][0][0])\n",
        "#             X2.append(in_seq)\n",
        "#             y.append(out_seq)\n",
        "#     # Make them arrays\n",
        "#     X1 = array(X1) \n",
        "#     X2 = array(X2, dtype='uint16') \n",
        "#     y  = array(y, dtype='uint16')\n",
        "#     # print('X1.shape:',X1.shape)\n",
        "#     # print('X2.shape:',X2.shape)\n",
        "#     # print('y.shape :',y.shape)\n",
        "#     # print()\n",
        "#     return X1, X2, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sT6jFXvzyk6"
      },
      "source": [
        "# def data_generator(captions, image_features, tokenizer, num_uniqe_words, max_length):\n",
        "#   # loop for ever over images.\n",
        "#   while 1:\n",
        "#     for key, captions_list in captions.items():\n",
        "#       # Get the image features\n",
        "#       single_image_features = images_features[key][0]\n",
        "#       print(key)\n",
        "#       print('single_image_features.shape:',single_image_features.shape)\n",
        "#       in_image, in_seq, out_word = create_sequences_generator(captions_list, \n",
        "#                                                               single_image_features,\n",
        "#                                                               tokenizer, \n",
        "#                                                               num_uniqe_words, \n",
        "#                                                               max_length)\n",
        "#       yield [[in_image, in_seq], out_word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvXu-IT0JaJL"
      },
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi5ldxkGJaJb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "72409b1d-7110-4858-bfc2-41d64bfc73f8"
      },
      "source": [
        "# from tensorflow.keras.layers.merge import add\n",
        "# from tensorflow.keras.layers.merge import concatenate\n",
        "'''As of keras 2, the module keras.layers.merge doesn't have a generic public Merge-Layer. \n",
        "Instead you are supposed to import the subclasses like keras.layers.Add or keras.layers.Concatenate etc. \n",
        "directly (or their functional interfaces with the same names lowercase: keras.layers.add, \n",
        "keras.layers.concatenate etc.). See what types of merging layers exist in the keras docs'''\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"As of keras 2, the module keras.layers.merge doesn't have a generic public Merge-Layer. \\nInstead you are supposed to import the subclasses like keras.layers.Add or keras.layers.Concatenate etc. \\ndirectly (or their functional interfaces with the same names lowercase: keras.layers.add, \\nkeras.layers.concatenate etc.). See what types of merging layers exist in the keras docs\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlUEmaKtJaJs"
      },
      "source": [
        "# define the captioning model\n",
        "def define_model(vocab_size, max_length):\n",
        "    # feature extractor model\n",
        "    inputs1 = Input(shape=(2048,))\n",
        "    fe1 = Dropout(0.5)(inputs1)\n",
        "    fe2 = Dense(256, activation='relu')(fe1)\n",
        "    # sequence model\n",
        "    inputs2 = Input(shape=(max_length,))\n",
        "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
        "    se2 = Dropout(0.5)(se1)\n",
        "    se3 = LSTM(256)(se2)\n",
        "    # decoder model\n",
        "    decoder1 = tf.keras.layers.add([fe2, se3])\n",
        "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "    # tie it together [image, seq] [word]\n",
        "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam',    )\n",
        "    # summarize model\n",
        "#     print(model.summary())\n",
        "    plot_model(model, to_file='model.png', show_shapes=True)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-2sV8h84rYY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c820a6d2-4187-4e34-f18b-100b430e574a"
      },
      "source": [
        "# Parse annotations data\n",
        "desc_dict = parse_annotations(annotations_file_path, \n",
        "                              percentage_of_selected_images=0.2)\n",
        "\n",
        "# Load images features\n",
        "images_features = load_images_features(desc_dict.keys())\n",
        "\n",
        "# Split data \n",
        "captions_train, captions_valid, \\\n",
        "images_features_train, images_features_valid = split_data(desc_dict, \n",
        "                                                          images_features, \n",
        "                                                          train_split_percent=0.8)\n",
        "\n",
        "# Create tokenizer\n",
        "tokenizer, num_uniqe_words, max_length = create_tokenizer(captions_train)\n",
        "\n",
        "vocabs_size = num_uniqe_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 16557 images.\n",
            "\n",
            "Cleaned descriptions are saved in 'cleaned_descriptions.json' file.\n",
            "\n",
            "Loading images features...\n",
            "Images features loaded!\n",
            "Single image features shape is (1, 2048)\n",
            "\n",
            "Splitting data...\n",
            "train_set_size: 13246\n",
            "valid_set_size: 3311\n",
            "train_images_ids size : 13246\n",
            "valid_images_ids size : 3311\n",
            "captions_train size       : 13246\n",
            "images_features_train size: 13246\n",
            "captions_valid size       : 3311\n",
            "images_features_valid size: 3311\n",
            "--------------------------\n",
            "There are 10398 unique words in the training dataset.\n",
            "COCO_train2014_000000449712\n",
            "The longest caption is 36 words length.\n",
            "Tokenizer created!\n",
            "--------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQBba1-ICLSS"
      },
      "source": [
        "  # epochs = 20\n",
        "  # batch_size_train = 100\n",
        "  # batch_size_valid = 100\n",
        "  # num_images_per_batch_train = round(len(captions_train)/batch_size_train)\n",
        "  # num_images_per_batch_valid = round(len(captions_valid)/batch_size_valid)\n",
        "  # steps_per_epochs_train = num_images_per_batch_train\n",
        "  # steps_per_epochs_valid = num_images_per_batch_valid\n",
        "  # print('Num. of training images  :',len(captions_train))\n",
        "  # print('Num. of validation images:',len(captions_valid))\n",
        "  # print('Num. of training images per batch  :',num_images_per_batch_train)\n",
        "  # print('Num. of validation images per batch:',num_images_per_batch_valid)\n",
        "  # print('num_epochs:', epochs)\n",
        "  # print('steps_per_epochs_train:',steps_per_epochs_train)\n",
        "  # print('steps_per_epochs_valid:',steps_per_epochs_valid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1VYW40aP3AU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af312320-2539-484f-fa49-e291b02d1952"
      },
      "source": [
        "?model.fit"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Object `model.fit` not found.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlvQVQNCJaNV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a140da41-07ca-4e3c-de64-2197d6bd87a6"
      },
      "source": [
        "import math\n",
        "# batch size = 2,4,8,16, 32,64, 128,256,512,1024,2048,4096,8192,16384 (2^n)\n",
        "if PROGRESSIVE_LOADING == 1:\n",
        "  # Define model\n",
        "  model = define_model(vocabs_size, max_length)\n",
        "  # Define fitting hyperparameters\n",
        "  epochs = 20\n",
        "  batch_size = [128] #[16, 32, 64, 128, 256, 512, 1024]\n",
        "  for BS in batch_size:\n",
        "    batch_size_train = BS\n",
        "    batch_size_valid = BS\n",
        "    # num_images_per_batch_train = 7\n",
        "    # num_images_per_batch_valid = 7\n",
        "    # batch_size_train = math.ceil(len(captions_train)/num_images_per_batch_train)\n",
        "    # batch_size_valid = math.ceil(len(captions_valid)/num_images_per_batch_valid)\n",
        "    num_images_per_batch_train = round(len(captions_train)/batch_size_train)\n",
        "    num_images_per_batch_valid = round(len(captions_valid)/batch_size_valid)\n",
        "    # steps_per_epochs_train = num_images_per_batch_train\n",
        "    # steps_per_epochs_valid = num_images_per_batch_valid\n",
        "    print('Num. of training images  :',len(captions_train))\n",
        "    print('Num. of validation images:',len(captions_valid))\n",
        "    print('Num. of training images per batch  :',num_images_per_batch_train)\n",
        "    print('Num. of validation images per batch:',num_images_per_batch_valid)\n",
        "    print('Batch size train:',batch_size_train)\n",
        "    print('Batch size valid:',batch_size_valid)\n",
        "    print('Num. of epochs:', epochs)\n",
        "    # print('steps_per_epochs_train:',steps_per_epochs_train)\n",
        "    # print('steps_per_epochs_valid:',steps_per_epochs_valid)\n",
        "    # Loop for fitting\n",
        "    loss     = list()\n",
        "    val_loss = list()\n",
        "    for i in range(epochs):\n",
        "      print('Epoch #',i+1)\n",
        "      # Create the data generator\n",
        "      generator_train = data_generator(captions_train, images_features_train,\n",
        "                                       tokenizer, num_uniqe_words,\n",
        "                                       max_length, num_images_per_batch_train)\n",
        "    \n",
        "      generator_valid = data_generator(captions_valid, images_features_valid,\n",
        "                                       tokenizer, num_uniqe_words,\n",
        "                                       max_length, num_images_per_batch_valid)\n",
        "      # Fit for one epoch\n",
        "      history = model.fit(x=generator_train, epochs=1, verbose=1,\n",
        "                          steps_per_epoch=batch_size_train,\n",
        "                          validation_data=generator_valid,\n",
        "                          validation_steps=batch_size_valid,\n",
        "                          max_queue_size=1, workers=1,\n",
        "                          use_multiprocessing=False)\n",
        "      \n",
        "      loss.append(round(history.history['loss'][0],4))\n",
        "      val_loss.append(round(history.history['val_loss'][0],4))\n",
        "      # Save model\n",
        "      model.save('MODELS/model_BS'+ str(batch_size_train) + '_EP' + str(i+1) + \\\n",
        "                      '_loss' + str(loss[i]) + '_val-loss' + str(val_loss[i]) + '.h5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num. of training images  : 13246\n",
            "Num. of validation images: 3311\n",
            "Num. of training images per batch  : 103\n",
            "Num. of validation images per batch: 26\n",
            "Batch size train: 128\n",
            "Batch size valid: 128\n",
            "Num. of epochs: 20\n",
            "Epoch # 1\n",
            "128/128 [==============================] - 76s 596ms/step - loss: 5.5737 - val_loss: 4.7139\n",
            "Epoch # 2\n",
            "128/128 [==============================] - 75s 584ms/step - loss: 4.2756 - val_loss: 3.9775\n",
            "Epoch # 3\n",
            "128/128 [==============================] - 74s 578ms/step - loss: 3.7499 - val_loss: 3.6968\n",
            "Epoch # 4\n",
            "128/128 [==============================] - 74s 580ms/step - loss: 3.4846 - val_loss: 3.5597\n",
            "Epoch # 5\n",
            "128/128 [==============================] - 75s 583ms/step - loss: 3.3105 - val_loss: 3.4902\n",
            "Epoch # 6\n",
            "128/128 [==============================] - 74s 579ms/step - loss: 3.1845 - val_loss: 3.4528\n",
            "Epoch # 7\n",
            "128/128 [==============================] - 74s 576ms/step - loss: 3.0859 - val_loss: 3.4295\n",
            "Epoch # 8\n",
            "128/128 [==============================] - 74s 577ms/step - loss: 3.0013 - val_loss: 3.4131\n",
            "Epoch # 9\n",
            "128/128 [==============================] - 74s 582ms/step - loss: 2.9328 - val_loss: 3.4038\n",
            "Epoch # 10\n",
            "128/128 [==============================] - 74s 581ms/step - loss: 2.8695 - val_loss: 3.3937\n",
            "Epoch # 11\n",
            "128/128 [==============================] - 74s 580ms/step - loss: 2.8136 - val_loss: 3.3928\n",
            "Epoch # 12\n",
            "128/128 [==============================] - 75s 582ms/step - loss: 2.7627 - val_loss: 3.3940\n",
            "Epoch # 13\n",
            "128/128 [==============================] - 75s 583ms/step - loss: 2.7168 - val_loss: 3.3966\n",
            "Epoch # 14\n",
            "128/128 [==============================] - 75s 587ms/step - loss: 2.6735 - val_loss: 3.4069\n",
            "Epoch # 15\n",
            "128/128 [==============================] - 74s 581ms/step - loss: 2.6389 - val_loss: 3.4152\n",
            "Epoch # 16\n",
            " 61/128 [=============>................] - ETA: 31s - loss: 2.6153"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-4cdfb7a3c317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m                           \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_valid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                           use_multiprocessing=False)\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SjRyqypdPpS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kdTnjQwhbJw"
      },
      "source": [
        "if PROGRESSIVE_LOADING != 1:\n",
        "  batch_size = 2**14\n",
        "  print('batch size:', batch_size)\n",
        "  epochs = 30\n",
        "  # Define the model\n",
        "  model = define_model(vocabs_size, max_length)\n",
        "  # define checkpoint callback\n",
        "  filepath = 'MODELS/' + 'model-' + 'batch_size' + str(batch_size) + \\\n",
        "    '-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "  # filepath = 'MODELS/model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
        "  checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "  # fit model\n",
        "  history = model.fit(x=[X1_train, X2_train],\n",
        "                      y=y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=epochs, \n",
        "                      verbose=1, \n",
        "                      callbacks=[checkpoint], \n",
        "                      validation_data=([X1_valid, X2_valid], y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OoHhwB9KAylP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11e2ad47-0b75-458a-80a9-5adef3844932"
      },
      "source": [
        "min(history.history['val_loss'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.3020920753479"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wxzUC_NJaNf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "7ca05443-26a1-46b5-fb6b-7d4324275e81"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for loss\n",
        "# plt.plot(history.history['loss'])\n",
        "# plt.plot(history.history['val_loss'])\n",
        "# plt.title('model loss')\n",
        "# plt.ylabel('loss')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper right')\n",
        "# plt.yscale('log')\n",
        "# plt.show()\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'val_loss'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAfX0lEQVR4nO3de7RV1WHv8e8PRBAFRTgmypFCjMlVsUXdMVpqhiFRUQixoTU2YGx6G3RER2mT+ODG2Ibk3hvTey3D1mjQmkeJEqu1oWqujyhp0sTHOUijBg1gyOBhA8H3A4L6u3/sdcL2sM/hLDjrHJDfZ4w9ztpzzrX2nDA4P9ZjzynbRERE9NSA/u5ARETsXhIcERFRSoIjIiJKSXBEREQpCY6IiCglwREREaUkOCIqJOkbkr7Uw7arJH1wZ48TUbUER0RElJLgiIiIUhIcsccrLhFdJOmnkl6W9I+S3ibpe5JelHSvpBEN7adJelzSc5IWSzqioe4YSUuK/b4DDOn0WVMlLS32/bGk393BPn9S0gpJz0haJOmQolyS/k7SekkvSHpU0vii7gxJPyv6tlbSZ3foDyz2eAmOiLrpwCnAu4APAd8D/gfQQv3fyV8ASHoXcBPwl0XdncC/Sdpb0t7AvwL/BBwI/HNxXIp9jwFuAM4DRgJfAxZJGlymo5ImAf8bOAs4GPglsLCoPhV4XzGO/Ys2G4u6fwTOsz0MGA/cV+ZzIzokOCLq/t72r2yvBX4IPGj7EdubgNuAY4p2HwXusH2P7S3A/wH2AX4fOAEYBMyzvcX2LcDDDZ8xC/ia7Qdtv277m8DmYr8yZgA32F5iezMwBzhR0lhgCzAM+G+AbC+z/XSx3xbgSEnDbT9re0nJz40AEhwRHX7VsP1qk/f7FduHUP8fPgC23wBWA6OLurV+88yhv2zY/h3gM8VlquckPQccWuxXRuc+vET9rGK07fuAfwCuBtZLmi9peNF0OnAG8EtJP5B0YsnPjQASHBFlraMeAED9ngL1X/5rgaeB0UVZhzEN26uB/2n7gIbXUNs37WQf9qV+6WstgO2rbB8HHEn9ktVFRfnDtj8MHET9ktrNJT83AkhwRJR1MzBF0gckDQI+Q/1y04+BnwCvAX8haZCkjwDHN+x7HXC+pPcWN7H3lTRF0rCSfbgJ+ISkCcX9kf9F/dLaKknvKY4/CHgZ2AS8UdyDmSFp/+IS2wvAGzvx5xB7sARHRAm2nwRmAn8P/Jr6jfQP2f6N7d8AHwH+FHiG+v2Qf2nYtw34JPVLSc8CK4q2ZftwL/B54FbqZzmHAWcX1cOpB9Sz1C9nbQT+tqg7B1gl6QXgfOr3SiJKUxZyioiIMnLGERERpSQ4IiKilARHRESUkuCIiIhS9urvDvSFUaNGeezYsf3djYiI3Up7e/uvbbd0Lt8jgmPs2LG0tbX1dzciInYrkn7ZrLzyS1WSBkp6RNLtTer+VNKGYrbQpZL+vKHuXEnLi9e5DeXHFTN+rpB0Vadv6UZERMX64oxjNrCM+heTmvmO7QsbCyQdCPw1UAMMtEtaZPtZ4BrqX6J6kPrMpJOpz2QaERF9oNIzDkmtwBTg+pK7ngbcY/uZIizuASZLOhgYbvuBYiK5bwFn9mqnIyKiW1WfccwDLqY+zXNXpkt6H/Bz4K9sd8w0urqhzZqibHSx3bl8G5JmUZ/GmjFjxjRrEhHRpS1btrBmzRo2bdrU312p3JAhQ2htbWXQoEE9al9ZcEiaCqy33S7p5C6a/Rtwk+3Nks4DvglM6o3Ptz0fmA9Qq9Uyr0pElLJmzRqGDRvG2LFjeSvfSrXNxo0bWbNmDePGjevRPlVeqpoITJO0ivrqZJMkLWhsYHtjsRAN1C9nHVdsr6U+VXWH1qJsbbHduTwioldt2rSJkSNHvqVDA0ASI0eOLHVmVVlw2J5ju9X2WOozd95ne2Zjm+KeRYdp1G+iA9wFnCppRLHW86nAXcVKZi9IOqF4murjwHerGkNE7Nne6qHRoew4+/x7HJLmAm22F1Fft2Aa9TUMnqGYYtr2M5K+yNZlN+fafqbY/hTwDerLdX6PPFEVEdGn+mTKEduLbU8tti8vQqPjrOQo279n+/22n2jY5wbb7yxeX28ob7M93vZhti/stExnRMRbwnPPPcdXv/rV0vudccYZPPfccxX0aKvMVRURsQvqKjhee+21bve78847OeCAA6rqFrCHTDkSEbG7ufTSS1m5ciUTJkxg0KBBDBkyhBEjRvDEE0/w85//nDPPPJPVq1ezadMmZs+ezaxZs4CtUyy99NJLnH766fzBH/wBP/7xjxk9ejTf/e532WeffXa6bwmOiIjt+MK/Pc7P1r3Qq8c88pDh/PWHjuqy/stf/jKPPfYYS5cuZfHixUyZMoXHHnvst4/M3nDDDRx44IG8+uqrvOc972H69OmMHDnyTcdYvnw5N910E9dddx1nnXUWt956KzNnzmz2caUkOCIidgPHH3/8m75ncdVVV3HbbbcBsHr1apYvX75NcIwbN44JEyYAcNxxx7Fq1ape6UuCIyJiO7o7M+gr++6772+3Fy9ezL333stPfvIThg4dysknn9z0exiDBw/+7fbAgQN59dVXe6UvuTkeEbELGjZsGC+++GLTuueff54RI0YwdOhQnnjiCR544IE+7VvOOCIidkEjR45k4sSJjB8/nn322Ye3ve1tv62bPHky1157LUcccQTvfve7OeGEE/q0b9oTvgZRq9WchZwiooxly5ZxxBFH9Hc3+kyz8Upqt13r3DaXqiIiopQER0RElJLgiIiIUhIcERFRSoIjIiJKSXBEREQpCY6IiF3Qjk6rDjBv3jxeeeWVXu7RVgmOiIhd0K4cHJV/c1zSQKANWNuxmFOTNtOBW4D32G6TNAO4qKHJ7wLH2l4qaTFwMNAx6cqpttdXNoCIiH7QOK36KaecwkEHHcTNN9/M5s2b+cM//EO+8IUv8PLLL3PWWWexZs0aXn/9dT7/+c/zq1/9inXr1vH+97+fUaNGcf/99/d63/piypHZ1NcSH96sUtKwos2DHWW2vw18u6g/GvhX20sbdpthO18Fj4i+8b1L4b8e7d1jvv1oOP3LXVY3Tqt+9913c8stt/DQQw9hm2nTpvHv//7vbNiwgUMOOYQ77rgDqM9htf/++3PllVdy//33M2rUqN7tc6HSS1WSWoEpwPXdNPsicAWw7dSOdX8CLOzlrkVE7Dbuvvtu7r77bo455hiOPfZYnnjiCZYvX87RRx/NPffcwyWXXMIPf/hD9t9//z7pT9VnHPOAi4FhzSolHQscavsOSRc1awN8FPhwp7KvS3oduBX4UtYdj4hKdXNm0BdsM2fOHM4777xt6pYsWcKdd97JZZddxgc+8AEuv/zyyvtT2RmHpKnAetvtXdQPAK4EPtPNMd4LvGL7sYbiGbaPBk4qXud0se8sSW2S2jZs2LCjw4iI6BeN06qfdtpp3HDDDbz00ksArF27lvXr17Nu3TqGDh3KzJkzueiii1iyZMk2+1ahyjOOicA0SWcAQ4DhkhbY7li3cBgwHlgsCeDtwCJJ0xruX5wN3NR4UNtri58vSroROB74VucPtz0fmA/12XF7e3AREVVqnFb99NNP52Mf+xgnnngiAPvttx8LFixgxYoVXHTRRQwYMIBBgwZxzTXXADBr1iwmT57MIYccUsnN8T6ZVl3SycBnu3qqqmizuGjTVrwfAKwGTrL9VFG2F3CA7V9LGkQ9VO61fW13n59p1SOirEyrvgtNqy5prqRpPWj6PmB1R2gUBgN3SfopsBRYC1xXQTcjIqILfbICoO3FwOJiu+mdG9snN9nnhE5lLwPHVdDFiIjooXxzPCKiC3vKA5tlx5ngiIhoYsiQIWzcuPEtHx622bhxI0OGDOnxPn1yqSoiYnfT2trKmjVr2BMe5x8yZAitra09bp/giIhoYtCgQYwbN66/u7FLyqWqiIgoJcERERGlJDgiIqKUBEdERJSS4IiIiFISHBERUUqCIyIiSklwREREKQmOiIgoJcERERGlJDgiIqKUBEdERJSS4IiIiFIqDw5JAyU9Iun2btpMl2RJteL9WEmvSlpavK5taHucpEclrZB0lSRVPYaIiNiqL6ZVnw0sA4Y3q5Q0rGjzYKeqlbYnNNnlGuCTRfs7gcnA93qttxER0a1KzzgktQJTgOu7afZF4ApgUw+OdzAw3PYDri/L9S3gzN7oa0RE9EzVl6rmARcDbzSrlHQscKjtO5pUjysucf1A0klF2WhgTUObNUVZs2PPktQmqW1PWMErIqKvVBYckqYC6223d1E/ALgS+EyT6qeBMbaPAT4N3Cip6aWurtieb7tmu9bS0lKy9xER0ZUqzzgmAtMkrQIWApMkLWioHwaMBxYXbU4AFkmq2d5seyNAETwrgXcBa4HGhXFbi7KIiOgjlQWH7Tm2W22PBc4G7rM9s6H+edujbI8t2jwATLPdJqlF0kAASe8ADgeesv008IKkE4qnqT4OfLeqMURExLb6/HsckuZKmradZu8DfippKXALcL7tZ4q6T1G/2b6C+plInqiKiOhDqj+c9NZWq9Xc1tbW392IiNitSGq3Xetcnm+OR0REKQmOiIgoJcERERGlJDgiIqKUBEdERJSS4IiIiFISHBERUUqCIyIiSklwREREKQmOiIgoJcERERGlJDgiIqKUBEdERJSS4IiIiFISHBERUUqCIyIiSklwREREKZUHh6SBkh6RdHs3baZLsqRa8f4USe2SHi1+Tmpou1jSk5KWFq+Dqh5DRERstVcffMZsYBkwvFmlpGFFmwcbin8NfMj2OknjgbuA0Q31M2xnLdiIiH5Q6RmHpFZgCnB9N82+CFwBbOoosP2I7XXF28eBfSQNrqyjERHRY1VfqpoHXAy80axS0rHAobbv6OYY04Eltjc3lH29uEz1eUnq4tizJLVJatuwYcOO9j8iIjqpLDgkTQXW227von4AcCXwmW6OcRT1s5HzGopn2D4aOKl4ndNsX9vzbdds11paWnZwFBER0VmVZxwTgWmSVgELgUmSFjTUDwPGA4uLNicAixpukLcCtwEft72yYyfba4ufLwI3AsdXOIaIiOiksuCwPcd2q+2xwNnAfbZnNtQ/b3uU7bFFmweAabbbJB0A3AFcavs/OvaRtJekUcX2IGAq8FhVY4iIiG31+fc4JM2VNG07zS4E3glc3umx28HAXZJ+CiwF1gLXVdvjiIhoJNv93YfK1Wo1t7Xl6d2IiDIktduudS7PN8cjIqKUBEdERJSS4IiIiFISHBERUUqCIyIiSklwREREKQmOiIgoJcERERGlJDgiIqKUBEdERJSS4IiIiFISHBERUUqCIyIiSklwREREKQmOiIgopUfBIWm2pOGq+0dJSySdWnXnIiJi19PTM44/s/0CcCowAjgH+HJPdpQ0UNIjkm7vps10Se5Yb7womyNphaQnJZ3WUD65KFsh6dIe9j8iInrJXj1sp+LnGcA/2X5ckrrbocFsYBkwvOmBpWFFmwcbyo6kvk75UcAhwL2S3lVUXw2cAqwBHpa0yPbPetiXiIjYST0942iXdDf14Lir+GX/xvZ2ktQKTAGu76bZF4ErgE0NZR8GFtrebPsXwArg+OK1wvZTtn8DLCzaRkREH+lpcPx34FLgPbZfAQYBn+jBfvOAi+kiZCQdCxxq+45OVaOB1Q3v1xRlXZU3O/YsSW2S2jZs2NCDrkZERE/0NDhOBJ60/ZykmcBlwPPd7SBpKrDednsX9QOAK4HPlOhvj9meb7tmu9bS0lLFR0RE7JF6GhzXAK9I+j3qv+hXAt/azj4TgWmSVlG/pDRJ0oKG+mHAeGBx0eYEYFFxg3wtcGhD29airKvyiIjoIz0Njtdsm/r9hH+wfTX1X/xdsj3HdqvtsdRvdN9ne2ZD/fO2R9keW7R5AJhmuw1YBJwtabCkccDhwEPAw8DhksZJ2rs47qIyA46IiJ3T06eqXpQ0h/pjuCcVl5kG7cgHSpoLtNnu8hd+8dTWzcDPgNeAC2y/Xux/IXAXMBC4wfbjO9KPiIjYMaqfSGynkfR24GPAw7Z/KGkMcLLt7V2u2iXUajW3tbX1dzciInYrktpt1zqX9+hSle3/Ar4N7F/c9N60u4RGRET0rp5OOXIW9XsMfwycBTwo6Y+q7FhEROyaenqP43PUv8OxHkBSC3AvcEtVHYuIiF1TT5+qGtARGoWNJfaNiIi3kJ6ecfw/SXcBNxXvPwrcWU2XIiJiV9aj4LB9kaTp1L/UBzDf9m3VdSsiInZVPT3jwPatwK0V9iUiInYD3QaHpBeBZl/0EGDbTadKj4iIt65ug8N2t9OKRETEnidPRkVERCkJjoiIKCXBERERpSQ4IiKilARHRESUkuCIiIhSEhwREVFKgiMiIkqpPDgkDZT0iKTbm9SdL+lRSUsl/UjSkUX5jKKs4/WGpAlF3WJJTzbUHVT1GCIiYqsez1W1E2YDy4Bm05PcaPtaAEnTgCuByba/TX3FQSQdDfyr7aUN+82wnbVgIyL6QaVnHJJagSnA9c3qbb/Q8HZfms+L9SfAwt7vXURE7IiqzzjmARcDXc55JekC4NPA3sCkJk0+Cny4U9nXJb1OfbbeL9neJnAkzQJmAYwZM2aHOh8REduq7IxD0lRgve327trZvtr2YcAlwGWdjvFe4BXbjzUUz7B9NHBS8Tqni+POt12zXWtpadmZoURERIMqL1VNBKZJWkX9UtMkSQu6ab8QOLNT2dlsXXUQANtri58vAjcCx/dWhyMiYvsqCw7bc2y32h5LPQDusz2zsY2kwxveTgGWN9QNAM6i4f6GpL0kjSq2BwFTgcazkYiIqFhfPFX1JpLmAm22FwEXSvogsAV4Fji3oen7gNW2n2ooGwzcVYTGQOBe4Lq+6XlERACoyX3lt5xarea2tjy9GxFRhqR227XO5fnmeERElJLgiIiIUhIcERFRSoIjIiJKSXBEREQpCY6IiCglwREREaUkOCIiopQER0RElJLgiIiIUhIcERFRSoIjIiJKSXBEREQpCY6IiCglwREREaUkOCIiopQER0RElFJ5cEgaKOkRSbc3qTtf0qOSlkr6kaQji/Kxkl4typdKurZhn+OKfVZIukqSqh5DRERs1RdnHLOBZV3U3Wj7aNsTgK8AVzbUrbQ9oXid31B+DfBJ4PDiNbmKTkdERHOVBoekVmAKcH2zetsvNLzdF+h2AXRJBwPDbT/g+mLp3wLO7KXuRkRED1R9xjEPuBh4o6sGki6QtJL6GcdfNFSNKy5x/UDSSUXZaGBNQ5s1RVmz486S1CapbcOGDTs1iIiI2Kqy4JA0FVhvu727dravtn0YcAlwWVH8NDDG9jHAp4EbJQ0v8/m259uu2a61tLTswAgiIqKZKs84JgLTJK0CFgKTJC3opv1CistOtjfb3lhstwMrgXcBa4HWhn1ai7KIiOgjlQWH7Tm2W22PBc4G7rM9s7GNpMMb3k4BlhflLZIGFtvvoH4T/CnbTwMvSDqheJrq48B3qxpDRERsa6++/kBJc4E224uACyV9ENgCPAucWzR7HzBX0hbq90fOt/1MUfcp4BvAPsD3ildERPQR1R9Oemur1Wpua2vr725EROxWJLXbrnUuzzfHIyKilARHRESUkuCIiIhSEhwREVFKgiMiIkpJcERERCkJjoiIKCXBERERpSQ4IiKilARHRESUkuCIiIhSEhwREVFKgiMiIkpJcERERCkJjoiIKCXBERERpVQeHJIGSnpE0u1N6s6X9KikpZJ+JOnIovwUSe1FXbukSQ37LJb0ZLHPUkkHVT2GiIjYqi+Wjp0NLAOGN6m70fa1AJKmAVcCk4FfAx+yvU7SeOAuYHTDfjNsZ0m/iIh+UOkZh6RWYApwfbN62y80vN0XcFH+iO11RfnjwD6SBlfZ14iI6JmqzzjmARcDw7pqIOkC4NPA3sCkJk2mA0tsb24o+7qk14FbgS+5ycLpkmYBswDGjBmzwwOIiIg3q+yMQ9JUYL3t9u7a2b7a9mHAJcBlnY5xFHAFcF5D8QzbRwMnFa9zujjufNs127WWlpadGElERDSq8lLVRGCapFXAQmCSpAXdtF8InNnxprjMdRvwcdsrO8ptry1+vgjcCBzf+12PiIiuVBYctufYbrU9FjgbuM/2zMY2kg5veDsFWF6UHwDcAVxq+z8a2u8laVSxPQiYCjxW1RgiImJbffFU1ZtImgu02V4EXCjpg8AW4Fng3KLZhcA7gcslXV6UnQq8DNxVhMZA4F7gur7sf0TEnk5N7iu/5dRqNbe15endiIgyJLXbrnUuzzfHIyKilARHRESUkuCIiIhSEhwREVFKgiMiIkpJcERERCkJjoiIKCXBERERpSQ4IiKilARHRESUkuCIiIhSEhwREVFKgiMiIkpJcERERCkJjoiIKCXBERERpSQ4IiKilMqDQ9JASY9Iur1J3fmSHpW0VNKPJB3ZUDdH0gpJT0o6raF8clG2QtKlVfc/IiLerC/OOGYDy7qou9H20bYnAF8BrgQoAuRs4ChgMvDVIoAGAlcDpwNHAn/SGDYREVG9SoNDUiswBbi+Wb3tFxre7gt0LID+YWCh7c22fwGsAI4vXitsP2X7N8DCom1ERPSRvSo+/jzgYmBYVw0kXQB8GtgbmFQUjwYeaGi2pigDWN2p/L1dHHcWMAtgzJgxO9D1iIhoprIzDklTgfW227trZ/tq24cBlwCX9dbn255vu2a71tLS0luHjYjY41V5qWoiME3SKuqXlCZJWtBN+4XAmcX2WuDQhrrWoqyr8oiI6COVBYftObZbbY+lfqP7PtszG9tIOrzh7RRgebG9CDhb0mBJ44DDgYeAh4HDJY2TtHdx3EVVjSEiIrZV9T2ObUiaC7TZXgRcKOmDwBbgWeBcANuPS7oZ+BnwGnCB7deL/S8E7gIGAjfYfryvxxARsSeT7e232s3VajW3tbX1dzciInYrktpt1zqX55vjERFRSoIjIiJK2SMuVUnaAPyyv/tR0ijg1/3diT6WMe8ZMubdx+/Y3ub7DHtEcOyOJLU1u7b4VpYx7xky5t1fLlVFREQpCY6IiCglwbHrmt/fHegHGfOeIWPezeUeR0RElJIzjoiIKCXBERERpSQ4+pGkAyXdI2l58XNEF+3OLdosl3Ruk/pFkh6rvsc7b2fGLGmopDskPSHpcUlf7tvel7O9ZY6LSTy/U9Q/KGlsQ13TpZN3dTs6ZkmnSGovlpJulzSp8767op35Oy7qx0h6SdJn+6rPvcJ2Xv30or5c7qXF9qXAFU3aHAg8VfwcUWyPaKj/CHAj8Fh/j6fqMQNDgfcXbfYGfgic3t9j6mKcA4GVwDuKvv4ncGSnNp8Cri22zwa+U2wfWbQfDIwrjjOwv8dU8ZiPAQ4ptscDa/t7PFWOt6H+FuCfgc/293jKvHLG0b8+DHyz2P4mW9cjaXQacI/tZ2w/C9xDfR12JO1HffXEL/VBX3vLDo/Z9iu27wdwfengJdTXZNkV9WSZ48Y/i1uAD0gSXS+dvKvb4THbfsT2uqL8cWAfSYP7pNc7bmf+jpF0JvAL6uPdrSQ4+tfbbD9dbP8X8LYmbUaz7XK5HcvofhH4v8ArlfWw9+3smAGQdADwIeD7VXSyF2x3DI1tbL8GPA+M7OG+u6KdGXOj6cAS25sr6mdv2eHxFv/puwT4Qh/0s9f1+XocexpJ9wJvb1L1ucY3ti2px89GS5oAHGb7rzpfN+1vVY254fh7ATcBV9l+asd6GbsiSUcBVwCn9ndfKvY3wN/Zfqk4AdmtJDgqZvuDXdVJ+pWkg20/LelgYH2TZmuBkxvetwKLgROBWrE0717AQZIW2z6ZflbhmDvMB5bbntcL3a1KT5Y57mizpgjD/YGNPdx3V7QzY0ZSK3Ab8HHbK6vv7k7bmfG+F/gjSV8BDgDekLTJ9j9U3+1e0N83WfbkF/C3vPlG8VeatDmQ+nXQEcXrF8CBndqMZfe5Ob5TY6Z+P+dWYEB/j2U749yL+k39cWy9cXpUpzYX8OYbpzcX20fx5pvjT7F73BzfmTEfULT/SH+Poy/G26nN37Cb3Rzv9w7syS/q13a/T32t9XsbfjnWgOsb2v0Z9RukK4BPNDnO7hQcOzxm6v+jM7AMWFq8/ry/x9TNWM8Afk79yZvPFWVzgWnF9hDqT9SsAB4C3tGw7+eK/Z5kF31yrDfHDFwGvNzw97oUOKi/x1Pl33HDMXa74MiUIxERUUqeqoqIiFISHBERUUqCIyIiSklwREREKQmOiIgoJcERsQuTdLKk2/u7HxGNEhwREVFKgiOiF0iaKekhSUslfU3SwGKdhb8r1g75vqSWou0ESQ9I+qmk2zrWJJH0Tkn3SvpPSUskHVYcfj9JtxTrkHy7Y3bViP6S4IjYSZKOAD4KTLQ9AXgdmAHsC7TZPgr4AfDXxS7fAi6x/bvAow3l3wautv17wO8DHbMIHwP8JfV1Ot4BTKx8UBHdyCSHETvvA8BxwMPFycA+1CdvfAP4TtFmAfAvkvYHDrD9g6L8m8A/SxoGjLZ9G4DtTQDF8R6yvaZ4v5T6FDM/qn5YEc0lOCJ2noBv2p7zpkLp853a7ej8Po3rUrxO/t1GP8ulqoid933qU2QfBL9dV/13qP/7+qOizceAH9l+HnhW0klF+TnAD2y/SH3q7TOLYwyWNLRPRxHRQ/mfS8ROsv0zSZcBd0saAGyhPp32y8DxRd166vdBAM4Fri2C4SngE0X5OcDXJM0tjvHHfTiMiB7L7LgRFZH0ku39+rsfEb0tl6oiIqKUnHFEREQpOeOIiIhSEhwREVFKgiMiIkpJcERERCkJjoiIKOX/Awqg3+IVJI24AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPoEBxNlJaNs"
      },
      "source": [
        "# map an integer to a word\n",
        "def word_for_id(integer, tokenizer):\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == integer:\n",
        "            return word\n",
        "    return None\n",
        "\n",
        "# generate a description for an image\n",
        "def generate_desc(model, tokenizer, image_features, max_length):\n",
        "    # seed the generation process\n",
        "    in_text = 'startseq'\n",
        "    # iterate over the whole length of the sequence\n",
        "    for i in range(max_length):\n",
        "        # integer encode input sequence\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        # pad input\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length)\n",
        "        # predict next word\n",
        "        yhat = model.predict([image_features, sequence], verbose=0)\n",
        "        # convert probability to integer\n",
        "        yhat = np.argmax(yhat)\n",
        "        # map integer to word\n",
        "        word = word_for_id(yhat, tokenizer)\n",
        "        # stop if we cannot map the word\n",
        "        if word is None:\n",
        "            break\n",
        "        # append as input for generating the next word\n",
        "        in_text += ' ' + word\n",
        "        # stop if we predict the end of the sequence\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    return in_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdfpbPQAJaN1"
      },
      "source": [
        "# descriptions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9d22xenJaOA"
      },
      "source": [
        "# # evaluate the skill of the model\n",
        "# from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# actual = list() \n",
        "# predicted = list()\n",
        "# # step over the whole set\n",
        "# for key, desc_list in tqdm(captions_valid.items()):\n",
        "#     # generate description\n",
        "#     yhat = generate_desc(model, tokenizer, images_features_valid[key][0], max_length)\n",
        "#     # store actual and predicted\n",
        "# #     print(desc_list[0])\n",
        "#     references = [d.split() for d in desc_list[0]]\n",
        "#     actual.append(references)\n",
        "#     predicted.append(yhat.split())\n",
        "# # calculate BLEU score\n",
        "# print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "# print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "# print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "# print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yc82YmoZJaOH"
      },
      "source": [
        "# evaluate the skill of the model\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def evaluate_model(model, descriptions, images_features, tokenizer, max_length):\n",
        "    actual, predicted = list(), list()\n",
        "    # step over the whole set\n",
        "    for key, desc_list in descriptions.items():\n",
        "        # generate description\n",
        "        yhat = generate_desc(model, tokenizer, images_features[key][0], max_length)\n",
        "        # store actual and predicted\n",
        "        references = [d.split() for d in desc_list]\n",
        "        actual.append(references)\n",
        "        predicted.append(yhat.split())\n",
        "    # calculate BLEU score\n",
        "    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n",
        "    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n",
        "    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n",
        "    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilYvmWa7JaOR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "0aa6cabd-8992-4ee6-982a-d8d33ae76a12"
      },
      "source": [
        "from keras.models import load_model\n",
        "# load the model\n",
        "filename = 'model-ep002-loss3.245-val_loss3.612.h5'\n",
        "model = load_model(filename)\n",
        "evaluate_model(model, captions_valid, images_features_valid, tokenizer, max_length)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-68-1941703962b4>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    evaluate model\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZ-BStszJaOY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZBHOTWYJaOj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es0cCod6JaOt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}